import openai

from openai import OpenAI
from typing import List, Tuple
import requests
import re

client = OpenAI(api_key='sk-proj-b8ymyvv75LlzBOisUYH8T3BlbkFJPeh2KB99bpIZ4WihDJQs')

# TODO: Add the range of the feature as a whole that is relevant to the task
# TODO: Add the reference count that supports this, and add examples (as links) to that references
# TODO: Add here a function call to other LLM (e.g: perplexity)
# TODO: How to scale this to more than 1 feature?
def evaluate_grouping(feature_name: str, groupings: List[Tuple[float, float]]) -> tuple:
    """
    Evaluates a grouping using GPT API and the crafted prompt, returning the grade and reason.

    Parameters:
        feature_name (str): The name of the feature.
        groupings (str): The groupings to evaluate, provided as a string.

    Returns:
        tuple: A tuple containing the grade (int) and the reason (str).
    """
    
    grouping_str = ", ".join([f"{low}-{high}" for low, high in groupings])
    
    # Define the crafted prompt
    prompt = f"""
    Prompt Template

    Part 1: Mission Context

    You are tasked with evaluating the semantic value of a given grouping for a specified feature. The goal is to assign a grade (1, 2, 3, or 4) based on how structured and meaningful the grouping is in relation to the feature. Your grading criteria are as follows:
        1. Grade 1 - Total Random:
    The groupings appear arbitrary, with no discernible structure or semantic meaning. They are not useful for analysis or interpretation.
        2. Grade 2 - Structured in Some Way:
    The groupings show some internal structure or logic but lack significant semantic meaning. They may make sense in a general context but are not clearly tied to meaningful distinctions.
        3. Grade 3 - It Has Significant Semantic Meaning in Some Contexts:
    The groupings reflect meaningful distinctions relevant to some real-world scenarios or domains. They are not standardized but provide value in specific applications.
        4. Grade 4 - This Is a Defined Objective Grouping:
    The groupings are based on universally recognized or objective standards, such as those established by scientific or governing bodies. They align with “objective truth.”

    You will use this context to evaluate the grouping provided.

    Part 2: The Feature

    Feature: {feature_name}

    Part 3: The Grouping

    Grouping: {grouping_str}

    Example Interaction

    Feature: Temperature in Celsius
    Grouping: -30–0, 1–15, 16–30, 31–50, 51+
    Response: Grade 2 - Structured in some way.

    Feature: Temperature in Celsius
    Grouping: ≤0, 1–35, 36–50, 51+
    Response: Grade 4 - This is a defined objective grouping.
    """

    # Call the GPT API
    try:
        response = client.chat.completions.create(model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an assistant that evaluates the semantic value of groupings based on structured criteria."},
                {"role": "user", "content": prompt}
        ])

        # Extract the GPT response
        gpt_response = response.choices[0].message.content.strip()

        # Parse the grade and reason
        grade = int(gpt_response.split("Grade ")[1][0])  # Extract the grade number
        reason = gpt_response.split("- ")[1].strip()     # Extract the reason
        return grade, reason

    except Exception as e:
        raise RuntimeError(f"Error during GPT API call: {e}")


def evaluate_grouping_perplexity(
    feature_name: str,
    groupings: List[Tuple[float, float]],
    feature_range: Tuple[float, float],
    perplexity_token: str
) -> tuple:
    """
    Evaluates a grouping using the Perplexity API and the requested prompt, returning
    the grade and explanation.

    Parameters:
        feature_name (str): The name of the feature.
        feature_range (Tuple[float, float]): The entire range of values for the feature (min, max).
        groupings (List[Tuple[float, float]]): The groupings to evaluate, provided as a list of (low, high) tuples.
        perplexity_token (str): Your Perplexity API token.

    Returns:
        tuple: A tuple containing:
            - grade (int): An integer (1, 2, 3, or 4) indicating how commonly the grouping is used.
            - explanation (str): The explanation or reasoning from the model's response.
    """

    # Convert the list of tuples into a readable string for the prompt
    grouping_str = ", ".join([f"{low}-{high}" if low != float('-inf') and high != float('inf') else f"lower than {high}" if low == float('-inf') else f"more than {low}" for (low, high) in groupings])

    # Define the new prompt
    prompt = f"""
ask context: You are tasked with evaluating the semantic value of a given grouping for a specified feature, by providing a metric of the number of times the provided grouping method is used with the stated feature.

You must also provide links to two of those references for validation purposes.

For a standardized metric you will use the following defined sources:

Google Scholar

PubMed

JSTOR

Office for National Statistics (ONS)

You will use this context to evaluate the grouping provided.

You will also add a level to this response based on how common the usage of a specific grouping is in the sources, picked from these 4 grades:

1. Not used at all
2. Has seen very few references to it
3. Rare but used
4. Very commonly used

Your reply template:
- Grade:
- Reference Count In Sources:
- Reference Example Links:
- Explanation:

Task:
Feature: {feature_name}
Range: {feature_range}
Grouping: {grouping_str}
    """

    # Build the request payload for Perplexity
    url = "https://api.perplexity.ai/chat/completions"
    payload = {
        "model": "llama-3.1-sonar-small-128k-online",
        "messages": [
            {
                "role": "system",
                "content": "Be precise and concise."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        # Optional parameters you can tune
        "max_tokens": 300,
        "temperature": 0.2,
        "top_p": 0.9,
        "search_domain_filter": [],         # e.g. ["perplexity.ai"] or leave empty
        "return_images": False,
        "return_related_questions": False,
        "search_recency_filter": "month",   # e.g. "month", "year", or "all"
        "top_k": 0,
        "stream": False,
        "presence_penalty": 0,
        "frequency_penalty": 1
    }

    headers = {
        "Authorization": f"Bearer {perplexity_token}",
        "Content-Type": "application/json"
    }

    # Send the request to the Perplexity API
    response = requests.post(url, json=payload, headers=headers)

    # Basic error handling for non-200 responses
    if response.status_code != 200:
        raise RuntimeError(
            f"Perplexity API returned status code {response.status_code}: {response.text}"
        )

    data = response.json()
    # The assistant's reply text (where we expect the grading format)
    try:
        model_response = data["choices"][0]["message"]["content"].strip()
    except (KeyError, IndexError) as e:
        raise RuntimeError(
            f"Unexpected response format from Perplexity: {data}"
        ) from e

    # Extract the grade (an integer 1-4) from the line beginning with "- Grade:"
    perplexity_pattern = re.compile(
    r"(?s)- \*\*Grade:\*\*\s*(\d+)(?:[^\r\n]*)?\s*?"
    r"- \*\*Reference Count In Sources:\*\*\s*([\s\S]*?)\s*?"
    r"- \*\*Reference Example Links:\*\*\s*([\s\S]*?)\s*?"
    r"- \*\*Explanation:\*\*\s*(.*)"
)
    match = perplexity_pattern.search(model_response)
    if match:
        grade = int(match.group(1))  # Convert the captured numeric string to int
        reference_count = match.group(2).strip()
        reference_links = match.group(3).strip()
        explanation     = match.group(4).strip()
    else:
        print("No match found in:\n" + model_response)
        raise RuntimeError("No match found in the model response.")


    return grade, explanation, reference_count, reference_links


def main():
    """
    Main function to test the evaluate_grouping function with predefined examples.
    """
    # Predefined features and groupings
    test_cases = [
        ("Monthly Salary in Dollars", [(0, 1000), (1000, 10000), (11000, float('inf'))], (0, float('inf'))),
        ("Temperature in Celsius", [(-30, 0), (1, 15), (16, 30), (31, 50), (51, float('inf'))], (float('-inf'), float('inf'))),
        ("Temperature in Celsius", [(float('-inf'), 0), (1, 35), (36, 50), (51, float('inf'))], (float('-inf'), float('inf'))),
        ("Age in Years", [(0, 12), (13, 19), (20, 64), (65, 100)], (0,100)),
        ("Height in Centimeters", [(0, 100), (101, 150), (151, 200), (201, float('inf'))], (0, 230)),
        ("BMI", [(0, 18.5), (18.5, 24.9), (25, 29.9), (30, 50)], (0, 50)),
        ("BMI", [(0, 10), (11, 20), (21, 30), (31, 50)], (0, 50)),
        ("Temperature in Celsius", [(-50, -10), (-9, 0), (1, 30), (31, float('inf'))], (float('-inf'), float('inf'))),
        ("Temperature in Celsius", [(float('-inf'), 0), (0, 10), (10, 25), (25, float('inf'))], (float('-inf'), float('inf'))),
        ("Age in Years", [(0, 18), (19, 35), (36, 50), (51, 70), (71, 100)], (0,100)),
        ("Age in Years", [(0, 1), (2, 5), (6, 12), (13, 17), (18, 24), (25, 100)], (0,100))
    ]

    # Iterate through test cases and print results
    for feature, grouping, feature_range in test_cases:
        try:
            grade, explanation, reference_count, reference_links = evaluate_grouping_perplexity(feature, grouping, feature_range, "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")
            print(f"Feature: {feature}, Grouping: {grouping}\nGrade: {grade}, Explanation: {explanation}\nReference Count: {reference_count}\nReference Links: {reference_links}\n")
        except RuntimeError as e:
            print(f"Error evaluating feature '{feature}' with grouping '{grouping}': {e}\n")

main()



# For the next meeting:
# Try another LLM
# Write the problem statement in a formal way
# Get some kind of numerical metric for the count of references
# Get Links for the references and validate them
# Go over code TODOs
# Make presentation
# For start, work on a more objective features like BMI.

# For future:
# 2 features ranking